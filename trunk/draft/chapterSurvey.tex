\chapter{Literature Survey}
\label{sec:survey}
% this will contians only the finished work .... and my comments to view the old survey see the file
%draft_chaptersurvey 
%\section{Introduction }
%\label{sec:Introduction}
The history of pen and sketch based computing goes back to 1963 with Ivan Sutherland's SKETCHPAD \cite{sutherlandsketch}, which used a light pen to draw on the monitor to create circuit diagrams. Current technology and the increase in computing power have increased the abilities of such interfaces and allowed the implementation of fully sketching systems. Over the years, many other sketching based systems have been developed. Through this history most system focused on building the interactive interfaces to assist users while entering information to the computer. To demonstrate our system inspiration we will first present current research in the area of sketch recognition. In Section \ref{sec:preprocessing}, preprocessing and segmentation methods will be discussed in details. Later, symbol recognition methods will be presented and analyzed in Section \ref{sec:symbolrecogntion}. 


% i need to write a history then small introduction to the survey...
%  i have to state that the next  sections will generally group segmentation and symbol recognition task. 
\section{Preprocessing and Segmentation}
\label{sec:preprocessing}
  In a system that supports free hand sketching, sketching should be as natural as possible. This means that the user can draw symbols using single or multiple strokes without constrains. The goal of a sketch system is making the user feel that using the system is nearly the same as using a pencil and paper. But sketches are incomplete drawings that are sloppy and messy, for example figures \ref{fig:Overshootandundershoot} and \ref{fig:overstroked} show examples of messy sketches drawn by users. This led researchers to perform sort of preprocessing before starting the sketch recognition task itself. The next sections explain different preprocessing techniques. 
   
\begin{figure}
	\centering
		\includegraphics[scale=0.7]{../../neededfiles/Figures/Overshootandundershoot.eps}
	\caption[Neat and Sloppy Symbols] {A sloppy and neat versions of symbol \cite{threeproblmes23}.}
	\label{fig:Overshootandundershoot}
\end{figure}

\begin{figure}
	\centering
		\includegraphics[scale=0.7]{../../neededfiles/Figures/overstroked.eps}
	\caption[Example of an over traced stroke]{Example of an over traced stroke \cite{overtraced24}}
	\label{fig:overstroked}
\end{figure} 

 %The preprocessing can be noise removing, stroke beautification or segmentation.  %Because of this 
 Sezgin and Davis \cite{overtraced24} tried to solve the over traced strokes (see Figure \ref {fig:overstroked})using a thinning algorithm. In \cite{meanshift10,domainindependent17}, Yu uses mean shift procedure as a noise remover and preprocessing step to the recognition step. The mean shift procedure is simply shifting the point to the average of its neighborhoods so smoothing out the noise without changing the global structure of data. Gaussian filters was used to smooth curves \cite {Phoenix88}. These procedures may seem to remove noises in the sketch but it often treat vertices as noise which obscure vertex locations. \\
 %% now the third paragraph..... 
Beside points location information (x, y positions) researcher made uses of the extra information from using digital ink. Some as \cite{mulitstroke5,polygonfeedback31} used stroke speed information to detect critical points where it is stated that user slow down at those critical points. Other used curvature and directions data. %Others \cite {MilitaryCOA37,computationalmodel16,polygonfeedback31}, debated on the importance of the extra information and weather the temporal and speed information is significant. 

 Agar et al. \cite{polygonfeedback31} used the timestamp for each point and compute time difference between every two successive point to help him find corners as he argued that it provides more distinct maximum than the speed information. He mentions that the pointing device (for example the mouse) sampling rate is the reason for this phenomena. The points are sampled at regular time intervals while the pointing device is moving. There are no points while the pointing device is stationary. This leads to a nearly constant time difference between samples while the pen is moving and large difference while the pen is stationary. Contrary to speed information, where the user draws with variant speeds which tends to make the speed information include a lot of noise in the data.
  
Sezgin et al.\cite{earlyprocess} used both speed and direction information. Their system gives better results than using only speed or direction information. This is because the speed information tend to be noisy when drawing large symbols, on the other side when using only direction information the some corner are missed. Our system uses speed, direction and time difference information in its preliminary calculation step. This ensures that all the possible vertices are considered when the segmentation algorithm segments strokes. %both speed and direction information help detecting vertices when the 
 


  % third should speech on how systems define critical points (this paragrah should speak on the data collected (speed crital points, ... 

 After critical point detection most sketch system proceeds to label segments into simple geometric primitives (i.e line, arc, etc...).  A hybrid algorithm was introduced in \cite{earlyprocess} where different sets of segments are generated based on both curvature and speed dominant points, followed by choosing a segmentation with the least error from a generated hybrid set. However, this system is limited to recognizing only specific simple geometric shapes with a set of low level recognizers. Each low level recognizer is designed to recognize only one geometric shape using spatial and geometric information extracted from input stroke.  
 
  A genetic algorithm was used by \cite{CruveDivisionSwarm} to optimally divide digital curves into lines and curves. Chen et al. \cite{CruveDivisionSwarm} uses digital curves scanned from paper as input to the system. Also, they did not take advantage of the curvature or local geometric properties of the digital curve. Yin \cite{PolygonApproximationPSO} used PSO to convert digital curves into polygons, our system adopts  Yin \cite{PolygonApproximationPSO} method and tries to improve it by adding curvature and other local information while segmenting strokes to achieve better segmentations. %The next section presents the general particle swarm algorithm which is used in both our segmentation algorithms. % are presented in section \ref{subsubsec:Discreteparticleswarmalgorithm}
 
 
Yu \cite{meanshift10} introduced a \textit{feature area} for each primitive and then computed the segmentation error for different types of primitives based on the computed \textit{feature area}. Figure \ref{fig:featurearearc} shows the feature area of a line and circle in Yu system, the shaded area is the difference between the computed feature area and the assumption area. His system achieved good accuracy in simple shapes (square, ellipse,...etc) but did not perform well in complex shapes. 
 
 Paulson and Hammond \cite{Paleosketch08} introduced a set of low level recognizers that were reported to achieve 98\% accuracy. However, their system similar to all low level recognizers in the way that it identifies a small set of simple shapes.% A genetic algorithm was used by \cite{CruveDivisionSwarm} to optimally divide digital curves into lines and curves. Chen et al.\cite{CruveDivisionSwarm} uses digital curves scanned from paper as input to the system and did not take advantage of the curvature or local geometric properties of the digital curve.
 
Arvo \cite{Fluid25} Tried to combine fast morphing and basic recognition for the sketch drawn as the user draw it. It tries to move the points of strokes drawn by the user draw into one of the known shapes (ex: box, lines and circle). It uses a relaxation and least square method for circles and uses the string of forces for finding best-fit box. However, their system can only identify circle and rectangles drawn using a single stroke.  
\begin{figure}
	\centering
	\subfigure[Feature area of line an arc]{
		\includegraphics{images/featurearearc.eps}}
		\subfigure[Feature area of line]{
				\includegraphics{images/featureareline.eps}}
	\caption[Feature Area]{Feature Area \cite{meanshift10}}
	\label{fig:featurearearc}
\end{figure}

\section{Symbol Recognition}
\label{sec:symbolrecogntion}

%In this section, we review three different types of symbol recognition methods.
 Symbol recognition can be based on: strokes, global shape properties, and appearance \cite{Oltmans07}. The role of which each stroke plays in the structure of the sketched symbol is used to determine shapes in stroke based methods. In global shape properties approach, systems investigate general shape properties and its underlying strokes. Using shape properties relaxes the assumption and role of each stroke in the symbol but it does not truly represent the appearance of the symbol. Unlike appearance based methods where individual strokes are disregarded and they focus on the overall appearance of shapes. We choose to use the method based on shape property but added some geometrical and structure properties used in most stroke based approaches. Our method tries to avoid many of the problems faced by approaches based on individual strokes or global shape properties while trying to gain the additional information that the stroke based system use. In the next sections, we look into three different symbol recognition approaches, describe their advantages and challenges.  

\subsection{Stroke Based Recognition}
\label{sec:StrokebasedRecognition}
Stroke based approach is based on the assumption that each stroke has a specific role in representing the symbol. Stroke based methods examine each stroke as the user draw it to determine its role in the symbol. These methods are suited to interactive interfaces since users expected that system displays interpretation of the symbol after the stroke is drawn or after drawing a group of strokes. The majority of the research in sketch recognition has focused on stroke-based methods because of the corresponding emphasis on interactive interfaces. Most stroke based systems either represent symbols as gestures or   hierarchical shape. 

\subsubsection{Gesture Recognition}
\label{sec:GuestureRecognition}

Single stroke gestures recognition was the focus of the early work in symbol recognition \cite{gestureexample12,aideddesgin22,sketchinginterfaces2}.  Gestures are pen strokes drawn by the user that can immediately recognized by the system. The result was either creating an action (e.g. deleting a shape) or drawing a shape on the screen. These systems are based on how the user draw the shape rather than how it looks. 

One of the first systems in gestures recognition is Rubine's algorithm \cite{gestureexample12} . He used Gaussian statistical classifier to classify gestures.  Simple features as length of stroke, smoothness of stroke, the angle that the first part of the gesture is drawn at and properties of the bounding box of the stroke are used to train the classifier to recognize symbols. In our system, we combine Rubine features with others to include the information on how the user draws sketch while recognizing the symbols. In \cite{Long00}, Long shows how similarities between gestures can be analyzed.  He used this information to identify gestures that are likely to confuse the recognition system. This helped interface designers to choose gestures that are easily recognized. 

The main problem with gesture based recognition systems that they impose several constrains on how the user can draw. Gestures must be drawn using a specific order and direction to be recognized correctly. Moreover, in most cases gestures do not necessarily look like the symbols they represent (e.g. to represent a rectangle the user draws only the left edge of it). Because of these reasons, gestures recognition systems are not suitable for free hand drawn sketches. 

\subsubsection{Hierarchical Shape Descriptions}

A Hierarchical shape descriptions was widely used in sketch systems \cite{sketchunderstanding1,HierarchicalParsing7,Ladder30,AlvaradoFreedom42,napkinGross}. Symbols are represented using hierarchical levels. The lower levels represent simple geometric primitives such as lines, arc and ellipses. Intermediate levels shapes are composed of lower level parts and constrain or connections between them. For example, a triangle is described as three lines connected at their endpoints. Higher level shapes are constructed from intermediate and lower level shapes, for example, a diode is described as a line on the top of a triangle head (acute angle).

 Systems that employ such representation must first pre-process strokes to break them into geometric primitives (see section \ref{sec:preprocessing} for a review on preprocessing methods). Recognition can then be treated as sub graph matching problem.   
 
%Although not based on a formal structural model of shapes, another probabilistic approach by Szummer and Qi in [37] uses conditional random fields (CRFs) to propagate information about the labeling of one stroke fragment to its neighbors. This allows their system to assign labels to stroke fragments that cannot easily be identified in isolation and to regroup over-segmented fragments. This helps mitigate the difficulty of determining the exact granularity at which to perform fragmentation by using the context of surrounding fragments and a global interpretation of the sketch. They have applied their algorithm to the binary labeling problem of distinguishing boxes from connectors in organizational charts with good results, even in several highly ambiguous cases.
\paragraph{Sub-graph matching:}
\label{sec:GraphSearching}

Representing the symbol that the user draws as a graph transforms the classification problem into a graph-matching problem. In general, each user geometrical primitive is represented as a node of the graph and the relations between those nodes are represented as arc. Figure \ref{fig:squarescematic} shows a graph that is constructed by \cite{mulitstroke5} for a rectangle. To classify the symbols the graph constructed must be matched with the graphs in the database. 

\begin{figure}
	\centering
		\includegraphics[scale=0.7]{../../neededfiles/Figures/squarescematic.eps}
	\caption[Semantic Graph of a Square ]{Semantic Graph of a Square \cite{mulitstroke5}}
	\label{fig:squarescematic}
\end{figure}


In the worst case, sub-graph matching is an exponential problem and thus is expensive to compute. Researcher tried to reduce complexity by restricting the search to fragments that are spatially or temporally grouped, or by using other assumptions. Some of the common assumptions are that each stroke is only part of one symbol and that user must draw one shape before drawing the next\cite{physicalmeaning6}. These assumptions add constrains on how user can draw sketches and thus violates freely drawn sketches.  Moreover, the matching process in highly affected by segmentation results. Over or under fragmentation of stroke into geometric primitive complicates the matching process.  If a stroke is broken into too many fragments then there will be extra components that will not map to any part of the symbol. If the stroke is divided enough then there will be component of the shape that are not filled while matching. The different ways of segmenting the sketches worsen the already high cost graph matching. 

 To solve this problem, \cite{mulitstroke5} provided two methods to match graphs, a simpler one using restrictive assumptions and if it fails the system uses a longer more computationally expensive search.  . Figure \ref{fig:squarescematic} shows a semantic graph that is constructed by the trainable recognizer in   \cite{mulitstroke5} for a rectangle. The trainable recognizer construct the semantic network at train time with then match the symbol user draw with the networks in the database. To match the networks quickly the system assumes the user draws the symbol with the same order as in training set. A deep heuristic graph search will employed only if no match was found. % there was no match found.



 Several other systems tried to model the matching process as a probabilistic problem \cite{HMM53,SketchRead2007}. These systems allow matching of partially satisfied shapes to propagate down to re interpret low level fragmentation hypothesis. In SKETCREAD \cite{SketchRead2007}, a dynamically generated Bayesian network represents the shape fragments hypotheses. Higher level structure of shape fragments can cause re interpretation of the geometric primitives. For example, if one head of an arrow is initially labeled as an arc and the system fragments the shape as an arrow then the belief that this stroke is an arc is decrease and the belief that it is a line increase. Finally, the arrow structure is completed and it is fully recognized. 
 
 \paragraph{Visual Descriptive Languages:}
 \label{sec:VisualDescribtiveLanguages}
 
 Avoiding graph matching problems, research tried to use languages that describe shapes instead of graphs. Visual descriptive languages represent the shape with a set of  tokens representing geometric primitives or spatial relations between smaller geometric primitives. Figures \ref{fig:arrow1} and \ref{fig:arr2} show a regular arrow with 2 shafts and head and its definition in Ladder language \cite{Ladder30}. Shape recognition is then a question of parsing the sketch document into a recognizer that parse strokes according to the constructed grammar. 
 
 Several visual grammar were introduced \cite{statisticalparsing26,Ladder30,GenericHMM28}. \cite{Ladder30,GenericHMM28}  Used visual description language which will generate the information of the structure and constrain of each object classified. The system automatically generates Java code for the recognizer for each object. The generated code cycles through the strokes drawn to check if any of the recognizers generated had interpreted any symbols partially or completely.  These systems are less computational expensive than graph matching approach but they also operate based on the output of the segmentation process. Any error in the segmentation process leads to huge error while parsing the whole sketch. Moreover, symbols cannot share strokes between them which influence and restrict how the user draws symbols. 
 % complexity of the graph matching problem 
\begin{figure}
	\centering
		\subfigure[An Arrow]
			{\label{fig:arrow1}\includegraphics[scale=0.75]{../../neededfiles/Figures/arrowladderdef1.eps}}
			\hfill
		\subfigure[Arrow grammar definition]%{\cite{Ladder30}}
		 {\label{fig:arr2}\includegraphics[scale=0.88]{../../neededfiles/Figures/arrowladderdef2.eps}}
	\caption[Arrow Definition]{\cite{Ladder30}}
		\label{fig:arrowladderdef1}
\end{figure}
 
\subsection{Global Shape Properties Based Recognition}
\label{sec:GlobalFeaturesBasedRecognition}

Drawing back from properties of individual strokes several researcher tried to use shape properties on the whole shape to classify shapes. They used global features that try to define the entire shape properties. 

 Several systems \cite{DiagramOfflineConvexHull,MulitStrokeConvexHull} presented features like ratio of bounding box to convex hull area and the ratio of the perimeter to the area.  These global features can be used to distinguish between classes. For example, a rectangle can be distinguished from a triangle by looking to the ratio of area of bounding box to the area of the convex hull. The triangle ratio will be much lower than that of the rectangle. 
 
 Fonseca et al.\cite{Cali63} created a system using similar set of features, their features were used by a number of fuzzy logic rules to identify symbols. They also used Naïve Bayes model that was trained to classify shapes based on those features. 
 
%Zernike moments were used by Hse et al. in \cite{ToADD}. 
Hse et al. \cite{HeloiseBeautification,zernike61} proposed using the magnitudes of Zernike moments as a shape property. Zernike moments are orthogonal moments which define the distribution of point in the input space. The higher the order of the moment the finer the detail level the point distribution represents. Hse et al. \cite{HeloiseBeautification,zernike61} used a SVM classifier to train the features vector calculated from the moments. Early evaluation of Zernike moments showed that they are invariant to both reflection and rotation of the input shape \cite{ZerMomentOrthogonal}. In our system we used Zernike moments as part of our feature vector. A detailed comparison between Zernike moments and other features are demonstrated in Section \ref{sec:featuresComparisions}. 
 
 
 \begin{figure}[]
	\centering
		\subfigure{Ac Source}{	

		\includegraphics[scale=0.6]{images/acSource.eps}		\label{fig:acSource}}
 
 		\subfigure{Current Source}{	
 		        
		\includegraphics[scale=0.6]{images/CurrentSource.eps} \label{fig:CurrentSource}}
	\caption{Example of Current and AC Sources}
	\label{fig:ACandCurrentSources}
\end{figure}




 
 Systems based on Global shape properties distinguish shapes by learning the different properties each shape class tends to have. These properties do not depend on the order or number of strokes and thus are only based on appearance of the shape not how the user draws it. However, individual details of the shape are impossible to distinguish using these properties. For example, it is impossible to use to differentiate between a current source symbol (circle containing and arrow Figure \ref{fig:CurrentSource}) and a Ac Source (circle containing a wave Figure \ref{fig:acSource}).  Finer details of the symbols are not represented in this approach, thus shapes that have small's details or only significant conceptual variation are not distinguished using this method. These symbols require a system to represent different levels of shape details.  
  
\subsection{Appearance Based Recognition}
\label{sec:ApperanceBasedRecognition}

In the third approach, the recognition focuses on appearance of shapes contrary to individual strokes or global shape properties. Kara \cite{imagetrainable48} used this approach while presenting a system that matches the input shape into a one of the prototypes in the database. Figure \ref{fig:template} shows an example of prototypes of some of the shapes in the system. Input shapes are first pose and scale normalized then four image similarity measures are used to match input shapes to the database prototypes. He used two measures (Hausdor distance) to measure similarity and two coefficients (Yule, Tanimoto) to measure dissimilarity. His approach allows the system to recognize a shape using only a single training sample as it is based on the shape prototype. However, the shape prototype must represent all transformation and variation the input shape can have. Moreover, they restrict the image representation to 48x48 pixels due to computational reasons. This restriction leads to eliminating some of the fine details that may be needed to distinguish some shape classes. There results on isolated symbols seem promising but the system has not been tested on full sketch that tends to have high overlapping between symbols. 
\begin{figure}
	\centering
		\includegraphics[scale=0.7]{images/template.eps}
	\caption[Prototypes of Shapes]{Prototypes of Shapes \cite{imagetrainable48}}
	\label{fig:template}
\end{figure}

\subsection{Discussion}
\label{sec:discuss}

In all of these system only \cite{SketchRead2007} using stroke based recognition have been implemented to the fully create a system that free draw sketch and understand them. This is because the emphasis on the interactive side of the sketch system which result in large difficulties in processing free hand sketches. Methods based on gesture and global stroke properties produce systems that are dependent on assumptions about how strokes can be grouped. These systems assume that users will finish drawing a shape before moving to the next one \cite{multistrokeEvaluation,Cali63,geometrydomain49}. This let them to temporally group strokes and then recognizes them based on their strokes or global shape properties. \cite{AlvaradoDigital} argued that this assumption does not hold in natural sketches. 

Recognition based on strokes using hierarchical shape description is applied in \cite{SketchRead2007}. The principle problem in these systems is the computational complexity of the matching between shape description and geometric primitives produced in the segmentation process. Natural sketch are difficult to segment and group to create reliable fragment of geometric primitives. This is partially due to the increase of noise and other phenomena such as overlapped, over traced strokes and sharing of strokes between different symbols. Since these systems are dependable on the segmentation results, they either perform recognition using undependable segmentation results or consider different ways to segment the input sketch. 

To avoid most problems in such system, our system uses hybrid of both global shape and hierarchical description methods. Currently, our system recognize independent symbols. Each symbol is segmented and hierarchical features are computed.  Features that are representing the global description of the shape are also generated and appended to the hierarchical features. Recognition is done using SVM Linear classifier as it proved to be one of the best learning algorithms. 

\chapter{Particle Swarm Optimization}
\label{sec:ParticleSwramOptimization}

\section{Swarm Optimization Review}
\label{sec:ParticleSwramOptimizationReview}
%This seems to emerge of what is called 'collective intelligence' from often large simple agents.  
% start with swarm in general  .. say about flock , ant and usin in image processing 
% then speeak about PSO and  its firstly generated   (see article)
%mention that is best than ga (see thesis) 

Modeling how a biological swarm has been recently studied by biologist and computer scientists. Scientists in the field of artificial life try to understand how such "social animals" interact, achieve goals and evolve. Moreover, a swarm behavior that achieves some kind of swarm intelligence is increasingly interesting for engineers as they can apply it in optimization. Looking from a higher view, agents in a swarm are cooperating to achieve some purposeful behavior and achieve some common goal. A good example of this cooperation is ant groups looking for food source. A large group of simple independent agents seems to generate this behavior in what is called 'collective intelligence'. Each agent has a set of simple rules to follow, and according to the interaction of the entire group the objective is met\cite{swarmGeneral}. %with other neighbourhood agents. 

Reynolds \cite{swarmflock} created 'boid' which simulated the motion of a flock of birds. Each boid is a simple agent that navigates according to his perception and a set of basic rules.  Kube and Zhange \cite{antsimulation} introduced a simulation model for ant's behavior and used in cooperative transportation. Ants leaves a trail of its movement, so when an ant find food it cannot carry alone, it can easily trace back to the colony and then a group of ants can track the trail and bring the food back.   Kube and Zhange \cite{antsimulation,swarmGeneral} system seems to correctly models ants behavior with a methods of task modeling. 

Image processing is one of the disciplines that used Swarm Intelligence. Ant colony was widely used to segment objects in \cite{antSegmentation,antthreshold,SwarmMedical}. Since 1995 when Kennedy and Eberhart \cite{PSOFirst,PSO2} introduced particle swarm algorithm (PSO) the algorithm had been used in various image processing system as an optimization  algorithm \cite{SwarmImageProcessing,Swarmimageprocessing2,Colorquantization,SwarmMedical}.  PSO algorithm was used in \cite{ToADD} to classify pixel to simplify the object segmentation process. 
 
\section{Discrete Particle Swarm Algorithm}
\label{sec:ParticleSwarmAlgorithm}
%\section{Particle Swarm Algorithm}
%\label{PSO}
%What is particle swarm algorithm and how it was used in related researches. 
The main idea of \textit{Particle Swarm Algorithm (PSO)} is to represent each agent with a particle from the solution space \cite{PSOFirst}. Each agent moves the particle with a direction and velocity $v_{ij}$ based on equations \ref{eq:Swarm} \& \ref{eq:Swarm1}.
\begin{equation}
%\[
p_{ij}=p_{ij}+v_{ij},
%\
\label{eq:Swarm1}
\end{equation}
where $p_{ij}$ represent the $jth$ particle in the $ith$ agent and $v_{ij}$ is the velocity of the $jth$ particle in the $ith$ agent.
 %Equation [\ref{eq:Swarm}] shows how velocity and direction of each particle are computed
 \begin{equation}
v_{ij}  = v_{ij}  + c_1 r_1 (lbest_{ij}  - p_{ij} ) + c_2 r_2 (gbest_{ij}  - p_{ij} )
\label{eq:Swarm}
\end{equation}
 where $lbest_{ij}$ is the local best particle, $gbest_{ij}$ is the global best particle, $r_1$ \& $r_2$ are random variables and $c_1$ \& $c_2$ are the swarm system variables.
 After each iteration the global best $g_{best}$ particle and the agent local best $l_{best}$ particle are evaluated based on the maximum fitness functions of all particles in the solution space. The solution is found after achieving a specific number of iteration or after an error threshold is achieved.
Equation \ref{eq:descrite} is used to change the general swarm algorithm into binary particle (\textit{Discrete Particle Swarm Algorithm DPSO}) which handles particle values of either $0$ or $1$ \cite{PSODisceret}. 
 \begin{equation}
   P(i)\Leftarrow 
\{
\begin{array}{c} 
1 \quad \quad if\quad r_{3}>p_{i}  \\

0 \quad \quad if\quad r_{3}<p_{i} 
\label{eq:descrite}
\end{array}\}
\end{equation}
 where $p_{ij}$ is the numerical values of the particle and $r_{3}$ is a random variable.
  
